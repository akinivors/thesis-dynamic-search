import time
import numpy as np
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, field

from .statistics import StatisticsCollector
from .models import KEstimationModel, EnsembleModel

@dataclass
class SearchResult:
    id: str
    title: str
    score: float
    source_ids: List[int]

@dataclass
class ExecutionMetrics:
    k_final_requested: int
    k_vector_used: int = 0
    vectors_fetched: int = 0
    edges_traversed: int = 0
    unique_targets_found: int = 0
    iterations: int = 0
    elapsed_ms: float = 0.0
    target_achieved: bool = False
    extra: Dict[str, Any] = field(default_factory=dict)

class ExecutionResult:
    def __init__(self, results, metrics):
        self.results = results
        self.metrics = metrics

class AdaptiveKVectorToGraph:
    """
    Adaptive Engine that converts Vector Search -> Graph Traversal.
    Supports:
    1. Iterative Refinement (Upward Adaptation)
    2. Early Termination (Downward Adaptation) - NEW!
    """
    
    def __init__(
        self,
        engine,
        stats_collector: StatisticsCollector,
        k_model: Optional[KEstimationModel] = None,
    ):
        self.engine = engine
        self.stats = stats_collector
        self.k_model = k_model or EnsembleModel()
    
    def execute(
        self,
        query_vec: np.ndarray,
        k_final: int,
        edge_type: str = "RELATED",
        use_iterative: bool = True,
        max_iterations: int = 5
    ) -> ExecutionResult: 
        
        start_time = time.perf_counter()
        
        # 1. Get stats & Estimate K
        edge_stats = self.stats.get_statistics(edge_type)
        k_vector_initial = self.k_model.estimate_k_vector(k_final, edge_stats)
        
        metrics = ExecutionMetrics(
            k_final_requested=k_final,
            k_vector_used=k_vector_initial
        )
        
        # --- NEW LOGIC ---
        # If use_iterative=False, we set max_iterations=1 (single-shot, but still uses early termination)
        # If use_iterative=True, we use max_iterations=5 (full recovery)
        current_max_iterations = max_iterations if use_iterative else 1

        # All execution goes through the Bidirectional logic now
        results = self._execute_bidirectional(
            query_vec, k_final, k_vector_initial, current_max_iterations, metrics
        )
        # -----------------
            
        metrics.elapsed_ms = (time.perf_counter() - start_time) * 1000
        metrics.target_achieved = len(results) >= k_final
        metrics.unique_targets_found = len(results)
        
        # If the Single-Shot (max_iterations=1) failed, force a final iterative recovery
        # THIS IS THE FINAL HYBRID STEP TO REACH 100% RELIABILITY IN THE FAST MODE
        if not use_iterative and not metrics.target_achieved:
            print(f"\n[Recovery] Single-shot failed for Q={k_final}. Forcing iterative recovery...")
            recovery_results = self._execute_bidirectional(
                query_vec, k_final, metrics.vectors_fetched + 5, 4, metrics
            )
            # Update metrics with recovery results
            metrics.elapsed_ms = (time.perf_counter() - start_time) * 1000
            metrics.target_achieved = len(recovery_results) >= k_final
            metrics.unique_targets_found = len(recovery_results)
            results = recovery_results

        return ExecutionResult(results, metrics)

    def _execute_bidirectional(
        self,
        query_vec: np.ndarray,
        k_final: int,
        k_vector_initial: int,
        max_iterations: int, # Handles both iterative (5) and single-shot (1)
        metrics: ExecutionMetrics
    ) -> List[SearchResult]:
        """
        COMPLETE Bidirectional Logic: Checks one-by-one (Downward), 
        and calculates next fetch size (Upward) if max_iterations > 1.
        """
        unique_targets: Dict[int, SearchResult] = {}
        vectors_fetched_total = 0
        vectors_actually_processed = 0
        k_fetch_target = k_vector_initial
        iteration = 0
        
        iteration_history = []
        
        while len(unique_targets) < k_final and iteration < max_iterations:
            iteration += 1
            
            # 1. Determine how many NEW vectors to fetch (k_fetch_target is cumulative)
            amount_to_fetch_in_batch = k_fetch_target - vectors_fetched_total
            
            # If the estimate (k_fetch_target) didn't increase, we force a minimum batch for iteration
            if amount_to_fetch_in_batch <= 0 and iteration > 1:
                 amount_to_fetch_in_batch = max(5, k_final - len(unique_targets))
            
            # If nothing to fetch and nothing found, break loop
            if amount_to_fetch_in_batch <= 0 and iteration == 1: 
                 break
            
            # 2. Fetch Batch from FAISS (Efficient Batch Retrieval)
            total_k_needed = vectors_fetched_total + amount_to_fetch_in_batch
            D, I = self.engine.vector_index.search(query_vec.reshape(1, -1), total_k_needed)
            
            # Slice out just the new ones
            new_indices = I[0][vectors_fetched_total:]
            new_scores = D[0][vectors_fetched_total:]
            
            if len(new_indices) == 0:
                break # No more vectors exist
            
            # Update total *fetched* (network cost paid)
            vectors_fetched_total += len(new_indices)
            
            # 3. Process Incrementally (Early Termination)
            targets_found_in_batch = 0
            
            for i, source_id in enumerate(new_indices):
                # We actively checked this graph node
                vectors_actually_processed += 1 
                score = float(new_scores[i])
                
                # Graph Traversal and Collection
                targets = self._traverse(source_id)
                metrics.edges_traversed += len(targets)
                
                for target_id in targets:
                    if target_id not in unique_targets:
                        # NEW target found
                        unique_targets[target_id] = SearchResult(
                            id=target_id, score=score, source_ids=[source_id], title='Unknown'
                        )
                        targets_found_in_batch += 1
                    else:
                        # Existing target updated
                        unique_targets[target_id].source_ids.append(source_id)
                        unique_targets[target_id].score = min(unique_targets[target_id].score, score)

                # --- âš¡ EARLY EXIT (DOWNWARD ADAPTATION) ---
                if len(unique_targets) >= k_final:
                    break
            
            # Log history
            step_log = {
                'iteration': iteration,
                'vectors_fetched_cumulative': vectors_fetched_total,
                'vectors_processed_cumulative': vectors_actually_processed,
                'targets_found': len(unique_targets)
            }
            iteration_history.append(step_log)
            
            # If we broke early, we exit the WHILE loop too
            if len(unique_targets) >= k_final:
                break
            
            # 4. ITERATIVE REFINEMENT (UPWARD ADAPTATION) - Only if max_iterations > 1
            if max_iterations > 1:
                yield_rate = targets_found_in_batch / len(new_indices) if len(new_indices) > 0 else 0
                remaining = k_final - len(unique_targets)
                
                if yield_rate > 0:
                    # Estimate needed vectors based on yield
                    needed = remaining / yield_rate
                    # Increase k_fetch_target (which determines the batch size in the next loop)
                    k_fetch_target = vectors_fetched_total + int(needed * 1.5) 
                else:
                    # Exponential backoff if yield is 0
                    k_fetch_target = vectors_fetched_total * 2
                
                k_fetch_target = min(k_fetch_target, 500) # Safety cap
            
        metrics.iterations = iteration
        metrics.vectors_fetched = vectors_actually_processed # We report what we USED
        metrics.k_vector_used = k_vector_initial # Initial estimate used
        metrics.unique_targets_found = len(unique_targets)
        metrics.extra['iteration_history'] = iteration_history
        metrics.extra['efficiency_gain'] = vectors_fetched_total - vectors_actually_processed
        
        return sorted(unique_targets.values(), key=lambda x: x.score, reverse=False)[:k_final]

    def _traverse(self, source_id: int) -> List[int]:
        """Helper to get neighbors"""
        try:
            return list(self.engine.graph.neighbors(source_id))
        except:
            return []

    def _execute_single_shot(self, query_vec, k_final, k_vector, metrics):
        """Legacy single-shot logic (no early exit optimization needed for benchmark)"""
        # (This remains mostly the same, or we can use the bidirectional logic with max_iterations=1)
        # For simplicity, let's just reuse the bidirectional logic with iter=1
        return self._execute_bidirectional(query_vec, k_final, k_vector, 1, metrics)

    def _traverse(self, source_id: int) -> List[int]:
        """Helper to get neighbors"""
        try:
            return list(self.engine.graph.neighbors(source_id))
        except:
            return []